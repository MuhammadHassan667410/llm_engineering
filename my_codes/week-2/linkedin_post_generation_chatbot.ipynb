{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46cd84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()\n",
    "from litellm import completion\n",
    "from analyzer import fetch_and_analyze_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e81552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert LinkedIn post generator. Given the content provided, create an engaging LinkedIn post that highlights\n",
    " the key points and encourages interaction.Answer in Markdowns.The post should be professional, concise, and tailored to a LinkedIn audience.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_generator(message, files):\n",
    "    if files and len(files) > 0:\n",
    "        contents = []\n",
    "        for file_obj in files:\n",
    "            file_data = fetch_and_analyze_file(file_obj.name)\n",
    "            if file_data['extracted_content']:\n",
    "                contents.append(f\"File: {file_data['filename']}\\n{file_data['extracted_content']}\")\n",
    "        content = '\\n\\n'.join(contents) if contents else message\n",
    "    else:\n",
    "        content = message\n",
    "    \n",
    "    prompt = f\"\"\"My name is Muhammad Hassan.I am a student who is self learning AI engineering through udemy.\n",
    "    I am currently following Ed Donner's Udemy course of LLM Engineering.I always post my progress on Linkedin that \n",
    "    what i did these past days, which projects I made, what did i learned in these days.I want you to create a professional \n",
    "    LinkedIn post for my LinkedIn account.The model i am using is gpt-5-nano.Analyze the files and go check Ed Donner's LLM Engineering \n",
    "    course on Udemy and make post according to it and please create a good post according to the information provided:\\n{content}\\n\n",
    "    People know my introduction so you don't need to include it in the post.\"\"\"\n",
    "    \n",
    "    if message and content != message:\n",
    "        prompt += f\"\\n\\nAdditional instructions: {message}\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=\"openai/gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 111, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 391, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\fastapi\\routing.py\", line 290, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "  File \"C:\\Users\\muham\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\muham\\projects\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\muham\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\locks.py\", line 209, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\muham\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000017D914E5430 [unset]> is bound to a different event loop\n"
     ]
    }
   ],
   "source": [
    "files = gr.Files(label=\"Upload files (.py, .ipynb, .pdf, .txt, .md, etc.)\", file_types=[\".py\", \".ipynb\", \".pdf\", \".txt\", \".md\", \".json\", \".yaml\", \".yml\"])\n",
    "input_message = gr.Textbox(label=\"Your message:\", info=\"Please provide a message or upload files to generate a LinkedIn post.\", lines=7)\n",
    "output_response = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=post_generator,\n",
    "    title=\"LinkedIn Post Generation Chatbot\", \n",
    "    inputs=[input_message, files], \n",
    "    outputs=[output_response],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b835685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
